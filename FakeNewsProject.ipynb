{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df2e70b",
   "metadata": {},
   "source": [
    "# 真假新闻特征识别和分类预测（代码整理）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8daaebc",
   "metadata": {},
   "source": [
    "## （一）数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('Twitter_Analysis.csv',encoding=\"utf-8\", index_col=0)\n",
    "df.head()\n",
    "\n",
    "# 查看数据基础信息\n",
    "df.shape\n",
    "df.info()\n",
    "\n",
    "# 数据清洗\n",
    "# 缺失值处理\n",
    "df.isnull().sum()\n",
    "\n",
    "# 重复值处理\n",
    "df.duplicated().sum()\n",
    "\n",
    "# 删除不需要的列\n",
    "columns_to_drop = [\n",
    "    'ORG percent', 'NORP percent', 'GPE percent', 'PERSON percent',\n",
    "    'MONEY percent', 'DATA percent', 'CARDINAL percent', 'PERCENT percent',\n",
    "    'ORDINAL percent', 'LAW percent', 'PRODUCT percent', 'EVENT percent',\n",
    "    'TIME percent', 'LOC percent', 'WORK OF ART percent', 'QUANTITY percent', 'LANGUAGE percent'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# 查看清洗后的数据\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70f597",
   "metadata": {},
   "source": [
    "## （二）标题维度特征分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba76775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标题关键词词云分析\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext.Sparkconf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"TitleDataCleaning\").getOrCreate()\n",
    "\n",
    "# 从Spark读取数据\n",
    "df= spark.read.csv(\"data/Twitter_Analysis.csv\",head=True,inferSchema=True)\n",
    "\n",
    "# 停用词集合\n",
    "stop_words = set(spark.sparkContext.broadcast(ENGLISH_STOP_WORDS))\n",
    "\n",
    "# 定义UDF去除停用词\n",
    "def remove_stopwords(statement):\n",
    "    words = statement.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words.value]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "\n",
    "# 添加一个新列，去除停用词\n",
    "df = df.withColumn('statement_without_stopwords', remove_stopwords_udf(df['statement']))\n",
    "\n",
    "# 根据标签分割数据\n",
    "df_label_F = df.filter(df['BinaryNumTarget'] == 0)\n",
    "df_label_T = df.filter(df['BinaryNumTarget'] == 1)\n",
    "\n",
    "# 收集数据到driver端\n",
    "statment_label_F = df_label_F.select('statement_without_stopwords').rdd.flatMap(lambda x: x).collect()\n",
    "statment_label_T = df_label_T.select('statement_without_stopwords').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# 生成词云\n",
    "# 生成标签为0的假新闻标题关键词词云\n",
    "wc_label_F = WordCloud(background_color=\"black\", max_words=100, max_font_size=256, random_state=42, width=2000, height=2000)\n",
    "wc_label_F.generate(' '.join(statment_label_F))\n",
    "\n",
    "# 显示标签为0的假新闻标题关键词词云\n",
    "plt.imshow(wc_label_F, interpolation=\"bilinear\")\n",
    "plt.title('Word Cloud for Label 0(FALSE)')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 生成标签为1的真新闻标题关键词词云\n",
    "wc_label_T = WordCloud(background_color=\"black\", max_words=100, max_font_size=256, random_state=42, width=2000, height=2000)\n",
    "wc_label_T.generate(' '.join(statment_label_T))\n",
    "\n",
    "# 显示标签为1的真新闻标题关键词词云\n",
    "plt.imshow(wc_label_T, interpolation=\"bilinear\")\n",
    "plt.title('Word Cloud for Label 1(TRUE)')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ffe5d",
   "metadata": {},
   "source": [
    "## （三）内容维度特征分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章长短比较\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"HistogramExample\").getOrCreate()\n",
    "\n",
    "# 计算每个\"Word count\"和\"majority_target\"组合的计数\n",
    "grouped_df = df.groupBy(\"Word count\", \"majority_target\").count()\n",
    "\n",
    "# 收集数据到driver端\n",
    "data = grouped_df.collect()\n",
    "\n",
    "# 初始化matplotlib的图和轴\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 为每个majority_target绘制直方图\n",
    "for target in df.select(\"majority_target\").distinct().collect():\n",
    "    target_data = [row for row in data if row['majority_target'] == target[0]]\n",
    "    ax.hist([row['Word count'] for row in target_data], bins=100, label=target[0], alpha=0.5)\n",
    "\n",
    "# 设置图表标题和标签\n",
    "ax.set_title('Distribution of Article Length')\n",
    "ax.set_xlabel('Word count')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# 显示图例\n",
    "ax.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b527fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词性、标点分布比较\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"BarChartExample\").getOrCreate()\n",
    "\n",
    "# 选择相关的词性列\n",
    "pos_columns = ['present_verbs', 'past_verbs', 'adjectives', 'adverbs', 'adpositions', 'pronouns', 'TOs', 'determiners', 'conjunctions']\n",
    "\n",
    "# 选择相关的标点列\n",
    "punct_columns = ['dots', 'exclamation', 'questions', 'ampersand']\n",
    "\n",
    "# 准备数据：将真新闻和假新闻的词性和标点数据合并到一个 DataFrame 中\n",
    "pos_data = df[pos_columns + ['BinaryNumTarget']].groupBy('BinaryNumTarget').pivot('Category').sum('Count')\n",
    "punct_data = df[punct_columns + ['BinaryNumTarget']].groupBy('BinaryNumTarget').pivot('Category').sum('Count')\n",
    "\n",
    "# 将PySpark DataFrame转换为Pandas DataFrame，以便使用matplotlib绘制图表\n",
    "pos_data_pd = pos_data.toPandas()\n",
    "punct_data_pd = punct_data.toPandas()\n",
    "\n",
    "# 重置索引，以便绘制条形图\n",
    "pos_data_pd = pos_data_pd.reset_index()\n",
    "punct_data_pd = punct_data_pd.reset_index()\n",
    "\n",
    "# 创建词性条形图\n",
    "fig_pos, ax_pos = plt.subplots()\n",
    "pos_data_pd.groupby(['Category', 'BinaryNumTarget']).sum().unstack().plot(kind='bar', ax=ax_pos, color=['red', 'blue'])\n",
    "ax_pos.set_title('真假新闻的词性分布')\n",
    "ax_pos.set_xlabel('词性')\n",
    "ax_pos.set_ylabel('次数')\n",
    "\n",
    "# 创建标点条形图\n",
    "fig_punct, ax_punct = plt.subplots()\n",
    "punct_data_pd.groupby(['Category', 'BinaryNumTarget']).sum().unstack().plot(kind='bar', ax=ax_punct, color=['red', 'blue'])\n",
    "ax_punct.set_title('真假新闻的标点分布')\n",
    "ax_punct.set_xlabel('标点')\n",
    "ax_punct.set_ylabel('次数')\n",
    "\n",
    "# 显示条形图\n",
    "plt.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efd752",
   "metadata": {},
   "source": [
    "## （四）社会影响力维度特征分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 社会影响力比较\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "# 初始化 Spark Session\n",
    "spark = SparkSession.builder.appName(\"RadarChart\").getOrCreate()\n",
    "\n",
    "# 选择真假新闻的数据\n",
    "true_news = df.filter(col('BinaryNumTarget') == 1)\n",
    "false_news = df.filter(col('BinaryNumTarget') == 0)\n",
    "\n",
    "# 计算真假新闻的平均值\n",
    "true_news_avg = true_news.agg(*(mean(col(c)).alias(c) for c in ['followers_count', 'friends_count', 'favourites_count', 'normalize_influence',\n",
    "                                                                 'mentions', 'quotes', 'replies', 'retweets', 'favourites'])).collect()[0]\n",
    "false_news_avg = false_news.agg(*(mean(col(c)).alias(c) for c in ['followers_count', 'friends_count', 'favourites_count', 'normalize_influence',\n",
    "                                                                   'mentions', 'quotes', 'replies', 'retweets', 'favourites'])).collect()[0]\n",
    "\n",
    "# 将数据转换为 NumPy 数组并规范化到[0,1]区间\n",
    "true_news_avg = np.array([true_news_avg[i] for i in true_news_avg.keys()])\n",
    "false_news_avg = np.array([false_news_avg[i] for i in false_news_avg.keys()])\n",
    "max_values = np.max(np.vstack([true_news_avg, false_news_avg]), axis=0)\n",
    "true_news_avg = true_news_avg / max_values\n",
    "false_news_avg = false_news_avg / max_values\n",
    "\n",
    "# 特征标签\n",
    "labels = list(true_news_avg.keys())\n",
    "\n",
    "# 计算角度\n",
    "angles = np.linspace(0, 2 * pi, len(labels), endpoint=False).tolist()\n",
    "\n",
    "# 数据闭环\n",
    "true_news_avg = np.concatenate((true_news_avg, [true_news_avg[0]]))\n",
    "false_news_avg = np.concatenate((false_news_avg, [false_news_avg[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "# 初始化雷达图\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# 绘制真假新闻的雷达图\n",
    "ax.fill(angles, true_news_avg, color='blue', alpha=0.25)\n",
    "ax.fill(angles, false_news_avg, color='red', alpha=0.25)\n",
    "ax.plot(angles, true_news_avg, color='blue', linewidth=2, label='真新闻')\n",
    "ax.plot(angles, false_news_avg, color='red', linewidth=2, label='假新闻')\n",
    "\n",
    "# 设置雷达图的特征标签\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "# 显示雷达图\n",
    "plt.show()\n",
    "\n",
    "# 停止 Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2bc4d9",
   "metadata": {},
   "source": [
    "## （五）情感维度特征分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0165a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感极性特征分布比较\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as p\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# 连接到 MySQL 数据库\n",
    "def get_texts_from_db():\n",
    "    # 连接到 MySQL 数据库\n",
    "    conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user='root',\n",
    "        password='123456',\n",
    "        database='fakenews'\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT tweet, majority_target FROM tweets_data\")\n",
    "    data = cursor.fetchall()  # 获取所有文本和标签\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [(tweets_data[0], tweets_data[1]) for tweets_data in data]\n",
    "\n",
    "# 进行情感分析并返回每个文本的情感得分\n",
    "def analyze_sentiment(texts):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    analysis_results = []\n",
    "\n",
    "    for text, label in texts:\n",
    "        sentiment_score = sid.polarity_scores(text)  # 获取情感得分\n",
    "        sentiment_compound = sentiment_score['compound']  # 取 compound 得分作为情感得分\n",
    "\n",
    "        analysis_results.append({\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "            'polarity': sentiment_compound\n",
    "        })\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "# 生成情感分布图\n",
    "def plot_sentiment_distribution(df):\n",
    "    # 获取 TRUE 和 FAKE 标签的情感得分\n",
    "    x1 = df.loc[df['label'] == 'TRUE']['polarity']\n",
    "    x2 = df.loc[df['label'] == 'FALSE']['polarity']\n",
    "\n",
    "    # 设置组标签和颜色\n",
    "    group_labels = ['TRUE', 'FAKE']\n",
    "    colors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n",
    "\n",
    "    # 创建分布图\n",
    "    fig = ff.create_distplot([x1, x2], group_labels, colors=colors)\n",
    "    fig.update_layout(title_text='Polarity Distribution for TRUE vs FAKE', template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "# 生成情感分布的小提琴图\n",
    "def plot_sentiment_distribution2(df):\n",
    "    # 使用 Plotly 生成小提琴图\n",
    "    fig = p.violin(df, y='polarity', color=\"label\",\n",
    "                   violinmode='overlay',   # 设置为叠加模式\n",
    "                   template='plotly_white', # 设置模板为白色背景\n",
    "                   title=\"Sentiment Distribution for TRUE vs FAKE\",  # 添加图表标题\n",
    "                   labels={\"polarity\": \"Sentiment Score\", \"label\": \"News Type\"})  # 设置轴标签\n",
    "\n",
    "    fig.update_traces(meanline_visible=True)  # 显示小提琴图的中位数\n",
    "    fig.show()  # 显示图表\n",
    "\n",
    "# 从数据库中获取文本和标签\n",
    "texts_from_db = get_texts_from_db()\n",
    "\n",
    "# 对获取的文本进行情感分析\n",
    "analysis_results = analyze_sentiment(texts_from_db)\n",
    "\n",
    "# 将情感分析结果转化为 DataFrame\n",
    "df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# 绘制情感分布图\n",
    "plot_sentiment_distribution(df)\n",
    "plot_sentiment_distribution2(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a0e36",
   "metadata": {},
   "source": [
    "## （六）可信度维度特征分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可信度评分分布比较\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"CredibilityScoreAnalysis\").getOrCreate()\n",
    "\n",
    "# 将真新闻和假新闻的可信度评分分别提取出来\n",
    "true_news = df.filter(col(\"BinaryNumTarget\") == 1).select(\"cred\").rdd.flatMap(lambda x: x).collect()\n",
    "fake_news = df.filter(col(\"BinaryNumTarget\") == 0).select(\"cred\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 创建直方图来显示真新闻和假新闻的可信度评分分布\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(true_news, bins=30, density=True, alpha=0.5, color='green', label='True News')\n",
    "plt.hist(fake_news, bins=30, density=True, alpha=0.5, color='red', label='Fake News')\n",
    "plt.title('Distribution of Credibility Scores by News Type (Histogram)')\n",
    "plt.xlabel('Credibility Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# 创建密度图来显示真新闻和假新闻的可信度评分分布\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(true_news, shade=True, color='green', label='True News')\n",
    "sns.kdeplot(fake_news, shade=True, color='red', label='Fake News')\n",
    "plt.title('Density Plot of Credibility Scores by News Type')\n",
    "plt.xlabel('Credibility Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# 调整子图间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c41144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可信度评分与其他特征的关系比较\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.stat import Correlation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始化SparkSession\n",
    "spark = SparkSession.builder.appName(\"FeatureCorrelationAnalysis\").getOrCreate()\n",
    "\n",
    "# 选择数值型特征列进行相关性分析\n",
    "numeric_features = [\n",
    "    'cred', 'followers_count', 'friends_count', 'favourites_count',\n",
    "    'statuses_count', 'listed_count', 'BotScore', 'normalize_influence',\n",
    "    'quotes', 'replies', 'retweets', 'favourites', 'hashtags'\n",
    "]\n",
    "\n",
    "# 使用PySpark的ML库计算皮尔逊相关系数矩阵\n",
    "r1 = Correlation.corr(df, numeric_features, \"pearson\").head()\n",
    "\n",
    "# 提取相关系数矩阵\n",
    "pearson_corr = np.array(r1[0]).reshape(len(numeric_features), len(numeric_features))\n",
    "\n",
    "# 绘制热力图\n",
    "plt.figure(figsize=(20, 15))  # 设置图像大小\n",
    "sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', linewidths=0.5, \n",
    "            linecolor='white', square=True)\n",
    "plt.title('Correlation Heatmap between Features and Credibility Score')\n",
    "plt.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbad136",
   "metadata": {},
   "source": [
    "## （七）情感实时处理维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3210d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka生产者配置\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "\n",
    "# MySQL 数据库配置\n",
    "DB_HOST = '127.0.0.1'\n",
    "DB_USER = 'root'\n",
    "DB_PASSWORD = '123456'\n",
    "DB_NAME = 'fakenews'\n",
    "\n",
    "# Kafka 配置\n",
    "KAFKA_BROKER = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'news_topic'\n",
    "\n",
    "# 配置情感分析器\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 情感分析函数\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_score = sid.polarity_scores(text)\n",
    "    return sentiment_score['compound']\n",
    "\n",
    "# 连接数据库：从 tweet_data 表读取数据\n",
    "def fetch_data_from_db():\n",
    "    conn = mysql.connector.connect(\n",
    "        host=DB_HOST,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        database=DB_NAME\n",
    "    )\n",
    "    query = \"SELECT tweet, timestamp FROM tweet_data\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "# Kafka 生产者：将数据发送到 Kafka\n",
    "def send_to_kafka():\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[KAFKA_BROKER],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    df = fetch_data_from_db()\n",
    "    for _, row in df.iterrows():\n",
    "        sentiment_score = analyze_sentiment(row['tweet'])  # 获取情感得分\n",
    "        news_data = {\n",
    "            'tweet': row['tweet'],\n",
    "            'timestamp': row['timestamp'].isoformat(),\n",
    "            'sentiment_score': sentiment_score  # 加入情感得分\n",
    "        }\n",
    "        producer.send(KAFKA_TOPIC, value=news_data)\n",
    "        print(f\"Sent tweet to Kafka: {news_data['tweet']} | Sentiment: {sentiment_score}\")\n",
    "\n",
    "# 启动生产者\n",
    "send_to_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka消费者配置，进行实时情感计算和可视化\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Kafka 配置\n",
    "KAFKA_BROKER = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'news_topic'\n",
    "\n",
    "# Pyspark Streaming 配置\n",
    "conf = SparkConf().setAppName(\"KafkaSparkSentimentAnalysis\")\n",
    "sc = SparkContext(conf=conf)\n",
    "ssc = StreamingContext(sc, 5) # 批次间隔为5秒\n",
    "\n",
    "# 从Kafka读取流数据\n",
    "kafka_stream = KafkaUtils.createStream(ssc, KAFKA_BROKER, \"consumer-group\", {KAFKA_TOPIC: 1})\n",
    "\n",
    "# 初始化\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "timestamps = []\n",
    "sentiment_scores = []\n",
    "fig = go.Figure(\n",
    "    data=[go.Scatter(x=timestamps, y=sentiment_scores, mode='lines+markers')],\n",
    "    layout=go.Layout(\n",
    "        title=\"实时情感分析\",\n",
    "        xaxis_title=\"时间\",\n",
    "        yaxis_title=\"情感极性\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 情感分析函数\n",
    "def analyze_sentiment(rdd):\n",
    "    global timestamps, sentiment_scores\n",
    "    if not rdd.isEmpty():\n",
    "        json_data = rdd.map(lambda x: json.loads(x[1]))\n",
    "        \n",
    "        for record in json_data.collect():\n",
    "            timestamp = record['timestamp']\n",
    "            sentiment_score = analyzer.polarity_scores(record['text'])['compound']\n",
    "            timestamps.append(datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S'))\n",
    "            sentiment_scores.append(sentiment_score)\n",
    "            \n",
    "            fig.data[0].x = timestamps\n",
    "            fig.data[0].y = sentiment_scores\n",
    "            fig.update_layout(\n",
    "                title=\"实时情感分析\",\n",
    "                xaxis_title=\"时间\",\n",
    "                yaxis_title=\"情感极性\"\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "# 从Kafka流中获取消息并分析情感\n",
    "kafka_stream.map(lambda x: x[1]).foreachRDD(analyze_sentiment)\n",
    "\n",
    "# 启动流计算\n",
    "ssc.start()\n",
    "\n",
    "# 等待终止并在300秒（5分钟）后停止\n",
    "time.sleep(300)\n",
    "\n",
    "# 停止流处理\n",
    "ssc.stop(stopSparkContext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5eb26",
   "metadata": {},
   "source": [
    "## （八）机器学习模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate\n",
    "\n",
    "# 加载数据集并设置变量\n",
    "features = ['cred', 'BotScore', 'tweet']\n",
    "target = 'BinaryNumTarget'\n",
    "data = pd.read_csv('.../database.csv')\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# 将数据拆分成训练集、验证集和测试集（70:15:15）\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2018)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, stratify=temp_labels, random_state=2018)\n",
    "\n",
    "# 对推文文本进行分词和填充处理\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_data['tweet'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['tweet'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_data['tweet'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['tweet'])\n",
    "\n",
    "max_sequence_length = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 分别提取文本特征和数值特征\n",
    "train_text_features = train_padded\n",
    "val_text_features = val_padded\n",
    "test_text_features = test_padded\n",
    "\n",
    "train_numeric_features = train_data[['cred', 'BotScore']].values\n",
    "val_numeric_features = val_data[['cred', 'BotScore']].values\n",
    "test_numeric_features = test_data[['cred', 'BotScore']].values\n",
    "\n",
    "# 定义输入层：文本输入和数值输入\n",
    "text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "numeric_input = Input(shape=(2,), name='numeric_input')\n",
    "\n",
    "# 定义文本处理部分（使用 Bi-LSTM 处理文本）\n",
    "embedding_layer = Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length)(text_input)\n",
    "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
    "lstm_layer = LSTM(64)(lstm_layer)\n",
    "dropout_layer = Dropout(0.5)(lstm_layer)\n",
    "\n",
    "# 合并文本特征和数值特征\n",
    "combined = Concatenate()([dropout_layer, numeric_input])\n",
    "\n",
    "# 结合后的特征传入全连接层\n",
    "dense_layer = Dense(64, activation='relu')(combined)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# 定义模型\n",
    "model = Model(inputs=[text_input, numeric_input], outputs=output_layer)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit([train_text_features, train_numeric_features], train_labels, epochs=5, batch_size=32, validation_data=([val_text_features, val_numeric_features], val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96db771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试和结果可视化\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Concatenate\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 特征列和目标列\n",
    "features = ['cred', 'BotScore', 'tweet']\n",
    "target = 'BinaryNumTarget'\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('C:/Users/LRQ/Desktop/database.csv')\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# 数据集拆分\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2018)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, stratify=temp_labels, random_state=2018)\n",
    "\n",
    "# 对推文文本进行分词和填充处理\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_data['tweet'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['tweet'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_data['tweet'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['tweet'])\n",
    "\n",
    "max_sequence_length = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 分别提取文本特征和数值特征\n",
    "train_text_features = train_padded\n",
    "val_text_features = val_padded\n",
    "test_text_features = test_padded\n",
    "\n",
    "train_numeric_features = train_data[['cred', 'BotScore']].values\n",
    "val_numeric_features = val_data[['cred', 'BotScore']].values\n",
    "test_numeric_features = test_data[['cred', 'BotScore']].values\n",
    "\n",
    "# 加载保存的模型\n",
    "model = tf.keras.models.load_model('twitter_analysis_model.h5')\n",
    "\n",
    "# 对测试数据进行预测\n",
    "predictions = model.predict([test_text_features, test_numeric_features])\n",
    "predicted_labels = np.round(predictions)  # 四舍五入，得到预测标签\n",
    "\n",
    "# 计算性能指标\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)  # 准确率\n",
    "f1 = f1_score(test_labels, predicted_labels)  # F1分数\n",
    "recall = recall_score(test_labels, predicted_labels)  # 召回率\n",
    "precision = precision_score(test_labels, predicted_labels)  # 精确度\n",
    "\n",
    "# 打印性能指标\n",
    "print(f'准确率: {accuracy:.4f}')\n",
    "print(f'F1 分数: {f1:.4f}')\n",
    "print(f'召回率: {recall:.4f}')\n",
    "print(f'精确度: {precision:.4f}')\n",
    "\n",
    "# 创建混淆矩阵\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "nrows, ncols = cm.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        c = cm[i, j]\n",
    "        p = cm_normalized[i, j]\n",
    "        s = f'{c}\\n({p:.2%})'\n",
    "        annot[i, j] = s\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = sns.heatmap(cm_normalized, annot=annot, fmt='', cmap='Blues', cbar=False)\n",
    "ax.set_xlabel('预测标签')  # 设置x轴标签\n",
    "ax.set_ylabel('真实标签')  # 设置y轴标签\n",
    "ax.set_title('混淆矩阵')  # 设置标题\n",
    "class_names = ['类别 {}'.format(i) for i in range(cm.shape[0])]  # 类别名称\n",
    "ax.xaxis.set_ticklabels(class_names)\n",
    "ax.yaxis.set_ticklabels(class_names)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
